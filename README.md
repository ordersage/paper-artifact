# Artifact evaluation for the paper "Avoiding the Ordering Trap in Systems Performance Measurement"

This repository contains the instructions for reproducing the paper by Duplyain
et al. at ATC '23. Some code and data are contained in other repositories,
linked here; the purpose of this document is to collect all information for
evaluating the artifacts in one place. 

## Getting Started

We offer data that can be directly checked against that reported in the paper,
as well as instructions for running the Case Studies reported in the paper.

Checking the numbers is generally simple, and can be done by examining data
presented in this repository and examining a Jupyter notebook that we provide.
This Jupyter nodebook can be re-executed either by using the evaluator's own
installation of Jupyter or by running it using a link we provide to Google
Collab, which only requires a Google account.

Re-executing the Case Study experiments is considerably more involved. The
preferred way to do this is to have a CloudLab account; we provide a profile on
CloudLab that automates the set up of the experiment, and running on the same
hardware used for the paper is likely to reproduce the same ordering effects.

It can take a few days for CloudLab accounts to be approved, and each of the
three Case Studies can take a day or more to run, so we **encourage evaluators to
leave plenty of time**.

CloudLab is free for academic use, and can be found at https://cloudlab.us 

## Data, Code, And Other Artifacts Used in This Document

This README refers to code, data, and other resources located in several
places. We begin by listing all of them here.

#### OrderSage Code

https://github.com/ordersage/ordersage

This repository contains the code for OrderSage, our tool for identifying order-dependent performance effects in systems benchmarks. This is a stand-alone project that can be applied to systems benchmarking tasks on its own; in this document, it is used to run experiments the Case Studies.

#### Long-term CloudLab Performance Dataset

https://zenodo.org/record/7903144 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7903144.svg)](https://doi.org/10.5281/zenodo.7903144)

This dataset was collected over a period of more than nine months on CloudLab, and represents nearly 2.3 executions of 25 benchmarks on 1,700 machines. It is used to make many of the claims in the paper regarding the overall prevalence of order-dependent effects.
 
## Reproducing The Paper

This section is organized according to the figures and tables in the paper.


### Table 1:

**Requirements: None**

This table was produced from our survey of artifacts and evaluations as
described in Section 2 of the paper. This data was tablulated from a
spreadsheet that can be found at https://XXX

### Figures 2--6 and Table 2:

**Requirements: Jupyter notebook, either local or hosted on a public platform**

These results came from the long-term dataset described in Section 4 of the
paper. The dataset itself is available at https://zenodo.org/record/7903144

The figures can be reproduced from the dataset using the Jupyter notebook that
can be found at
https://colab.research.google.com/drive/1JSgLaILACBGWKQW4X0I6xxFgwpe4-yuN . You
can either run this notebook "in place" on Google Collab, or download it and
run it in your own installation of Jupyter.

Instructions for running the notebook in Google Collab can be found at the top
of the document.

* Figures 2-4 are labeled in the titles of the plots under the heading "Visualizations for the paper"
* Figure 5 is generated by cells XXX and YYY
* Figure 6 is generated by cells XXX and YYY
* Table 2 uses numbers that are generated by cell XXX

### Tables 3--5:

These tables were generated via experiments run by OrderSage. We offer two ways
of reproducing the tables: comparing them to the data we collected, and
re-running the experiments and collecting your own data.

#### Checking against our data

**Requirements: none**

The data collected in the experiments described in Section 7 of the paper can
be found in the XXX/ subdirectory of this repository. Each directory contains
files describing the configuration of OrderSage, the environments gathered from
the node(s) the experiments were run on. The data used to produce the tables
came from:

* Test name: file XXX, column YYY
* KW p-value: file XXX, column YYY
* KW test: file XXX, column YYY
* \delta %: file XXX, column YYY
* CI case: file XXX, column YYY

Other points of interest in these directories include:

* Make a
* list of some
* other things
* they may find interesting

Data for the tables can be found in the directories:

* Table 3: XXX
* Table 4: XXX
* Table 5: XXX

#### Re-running experiments on CloudLab

**Requirements: CloudLab account; multiple days of runtime**

If you wish to run your own experiments to collect data like the above, we
highly recommend doing so on CloudLab for two reasons. First, we have provided
a "profile" that makes the process simple. Second, because ordering effects can
be highly dependent on hardware, operating system, and other software
differences (as described in the paper), we would not necessarily expect to get
similar results in a different environment.

Our CloudLab profile is available at https://XXX

To run experiments for each table, you will:

* Instantiate the profile
* Log in to the "controller" node via ssh
* Clone the OrderSage profile via `git clone https://XXX`
* `cd` into the OrderSage directory (`cd ordersage`)
* Copy the configuration file for the experiment you will run (XXX describe)
* Run the experiment (XXX describe): **Important: this step can take more than 24 hours, depending on the experiment**
* Interpret the results in `XXX/` as described in the "checking against our data" section of this document
* Terminate your CloudLab experiment

Special instructions for:

* Table 3
* Table 4
* Table 5

We note that you are *not* expected to get results that are *identical* to the
ones we reported in the paper; there is enough randomness in this process that
results are not identical every time (which is indeed the main point of the
paper). What we do expect is that you should be get results that reach the same
conclusion as our paper.

#### Re-running experiments in other environments

**Requirements: At least two machines (you must have root on one and be able to reboot it frequently); dedicated NVMe drive required for Table 5; multiple days of runtime**

If you wish to run our experiments with OrderSage on your own environment, you will need two machines:

* A "controller" that will remain on and connected to the network for the duration of the experiment
* A "worker" that will be used to run tests. You must have root access on this worker, it must not be running other workloads, and the worker will get rebooted frequently during the process. To run the experiments for Table 5, the worker must have an NVMe drive that is dedicated to the experiment (ie. has no filesystem on it, and will be re-written as part of the experiment)

We note that you are *not* likely to get the same results as we did; a major
motivator for the paper was that different hardware, software, etc. can have 
different ordering effects.

You will need to set up OrderSage and its dependencies (including passwordless
ssh to the worker) as described in its repository: [https://github.com/ordersage/ordersage](https://github.com/ordersage/ordersage). Use the
OrderSage configuration files as described in the CloudLab section above, and
interpret results as described in the "checking against our data" section
above.

